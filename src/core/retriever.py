from langchain_community.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader, TextLoader
# Import Google Embeddings
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import os
import sys # Import sys for potential critical exits
from langchain.schema import Document # Keep this for manual loading
from typing import List, Optional # Add type hints for clarity
import re  # Th√™m th∆∞ vi·ªán re ƒë·ªÉ s·ª≠ d·ª•ng regex

# --- Constants ---
DEFAULT_DATA_DIR = "data"
GOOGLE_EMBEDDING_MODEL = "models/embedding-001"
TEXT_CHUNK_SIZE = 1000
TEXT_CHUNK_OVERLAP = 200
SEARCH_K = 4 # Default number of documents to retrieve

class DocumentRetriever:
    # Add type hints to __init__
    def __init__(self, data_dir: str = DEFAULT_DATA_DIR):
        self.data_dir = data_dir
        print(f"‚ÑπÔ∏è DocumentRetriever initialized with data directory: '{self.data_dir}'")

        # Validate Google API Key early
        if not os.getenv("GOOGLE_API_KEY"):
             print("‚ùå Error: GOOGLE_API_KEY not found in environment variables.")
             # Consider raising a more specific error or handling it based on application needs
             raise ValueError("GOOGLE_API_KEY not found in environment variables.")
        else:
            print("‚úÖ GOOGLE_API_KEY found.")

        # Initialize embeddings
        self.embedding_model_name = GOOGLE_EMBEDDING_MODEL
        try:
            self.embeddings = GoogleGenerativeAIEmbeddings(model=self.embedding_model_name)
            print(f"‚úÖ Google Embeddings initialized with model: '{self.embedding_model_name}'")
        except Exception as e:
            print(f"‚ùå Error initializing Google Embeddings: {e}")
            # Depending on the app, you might want to raise the error or handle it
            raise RuntimeError(f"Failed to initialize Google Embeddings: {e}") from e

        self.vector_store: Optional[FAISS] = None # Initialize vector_store with type hint

    # Add type hints
    def load_documents(self) -> List[Document]:
        """Load documents from the data directory using DirectoryLoader."""
        if not os.path.exists(self.data_dir):
            print(f"‚ö†Ô∏è Warning: Data directory '{self.data_dir}' does not exist.")
            return []

        print(f"‚ÑπÔ∏è Attempting to load documents from '{self.data_dir}' using DirectoryLoader...")
        try:
            # Using TextLoader with UTF-8 encoding
            loader = DirectoryLoader(
                self.data_dir,
                glob="**/*.txt", # Ensure this pattern matches your files
                loader_cls=lambda file_path: TextLoader(file_path, encoding="utf-8"),
                show_progress=True, # Optional: show progress
                # Use recursive=True if documents are in subdirectories
                recursive=True
            )
            documents = loader.load()
            if documents:
                print(f"‚úÖ Successfully loaded {len(documents)} documents using DirectoryLoader.")
            else:
                print(f"‚ÑπÔ∏è DirectoryLoader found no .txt files matching the pattern in '{self.data_dir}'.")
            return documents
        except Exception as e:
            print(f"‚ùå Error loading documents using DirectoryLoader: {e}")
            # Optionally log the full traceback for debugging
            # import traceback
            # print(traceback.format_exc())
            return [] # Return empty list on error

    # Add type hints
    def load_documents_manually(self) -> List[Document]:
        """Load documents manually, iterating through files in the data directory."""
        documents = []
        if not os.path.exists(self.data_dir):
            # This case is already handled by the calling function, but good for standalone use
            print(f"‚ö†Ô∏è Warning: Data directory '{self.data_dir}' does not exist.")
            return documents

        print(f"‚ÑπÔ∏è Attempting to load documents manually from '{self.data_dir}'...")
        loaded_count = 0
        error_count = 0
        try:
            for filename in os.listdir(self.data_dir):
                if filename.endswith('.txt'):
                    file_path = os.path.join(self.data_dir, filename)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        if content.strip(): # Ensure content is not just whitespace
                            doc = Document(
                                page_content=content,
                                metadata={"source": file_path} # Use filename as source
                            )
                            documents.append(doc)
                            loaded_count += 1
                        else:
                            print(f"‚ö†Ô∏è Skipping empty file: {filename}")
                    except Exception as e:
                        print(f"‚ùå Error loading file '{file_path}' manually: {e}")
                        error_count += 1

            if loaded_count > 0:
                print(f"‚úÖ Manually loaded {loaded_count} documents.")
            if error_count > 0:
                print(f"‚ö†Ô∏è Encountered errors loading {error_count} files manually.")
            if loaded_count == 0 and error_count == 0:
                 print(f"‚ÑπÔ∏è No .txt files found or loaded manually in '{self.data_dir}'.")

            return documents
        except Exception as e:
            print(f"‚ùå Error during manual document loading process: {e}")
            return [] # Return empty list on error

    # Add type hints
    def create_vector_store(self) -> Optional[FAISS]:
        """Create or retrieve the FAISS vector store from loaded documents."""
        # Check if already created
        if self.vector_store:
            print("‚ÑπÔ∏è Vector store already exists.")
            return self.vector_store

        print("--- Starting Vector Store Creation ---")
        # Try DirectoryLoader first
        documents = self.load_documents()
        # If DirectoryLoader fails or finds nothing, try manual loading
        if not documents:
             print("‚ÑπÔ∏è DirectoryLoader yielded no documents, attempting manual loading...")
             documents = self.load_documents_manually()

        # If still no documents, abort
        if not documents:
            print("‚ùå Error: No documents found after attempting both loading methods.")
            print(f"Ensure .txt files are present in '{self.data_dir}'.")
            self.vector_store = None # Explicitly set to None
            print("--- Vector Store Creation Failed ---")
            return None

        # Split documents
        print(f"‚ÑπÔ∏è Splitting {len(documents)} documents into chunks (size={TEXT_CHUNK_SIZE}, overlap={TEXT_CHUNK_OVERLAP})...")
        try:
            text_splitter = CharacterTextSplitter(
                chunk_size=TEXT_CHUNK_SIZE,
                chunk_overlap=TEXT_CHUNK_OVERLAP,
                separator="\n" # Common separator, adjust if needed
            )
            texts = text_splitter.split_documents(documents)
        except Exception as e:
             print(f"‚ùå Error splitting documents: {e}")
             print("--- Vector Store Creation Failed ---")
             return None


        if not texts:
             print("‚ùå Error: Failed to split documents into text chunks (result was empty).")
             print("--- Vector Store Creation Failed ---")
             return None
        print(f"‚úÖ Successfully split documents into {len(texts)} text chunks.")

        # Create FAISS index
        print(f"‚ÑπÔ∏è Creating FAISS vector store with {len(texts)} chunks using '{self.embedding_model_name}'...")
        try:
            vector_store = FAISS.from_documents(texts, self.embeddings)
            self.vector_store = vector_store # Store the created vector store
            print("‚úÖ FAISS vector store created successfully.")
            print("--- Vector Store Creation Finished ---")
            return vector_store
        except Exception as e:
            print(f"‚ùå Error creating FAISS vector store: {e}")
            # Check for common API key issues
            if "API key not valid" in str(e) or "permission" in str(e).lower():
                 print("‚ÄºÔ∏è Hint: Check if your GOOGLE_API_KEY is correct and has the necessary permissions for the embedding model.")
            self.vector_store = None # Ensure it's None on failure
            print("--- Vector Store Creation Failed ---")
            return None

    # Add type hints
    def search_documents(self, query: str, k: int = SEARCH_K) -> List[Document]:
        """Search for documents related to the query using the vector store."""
        print(f"\n--- Performing Search for query: '{query}' (k={k}) ---")
        # Ensure vector store exists, try creating if not
        if not self.vector_store:
            print("‚ö†Ô∏è Vector store not initialized. Attempting to create it now...")
            self.create_vector_store()
            # If creation failed, return empty list
            if not self.vector_store:
                 print("‚ùå Error: Failed to create vector store for search. Cannot perform search.")
                 print("--- Search Failed ---")
                 return []

        # Ki·ªÉm tra xem query c√≥ li√™n quan ƒë·∫øn c√°c th√†nh ph·ªë kh√¥ng
        city_patterns = {
            "hcm": r'(h·ªì\s*ch√≠\s*minh|s√†i\s*g√≤n|tp\s*hcm|tp\.?\s*h·ªì\s*ch√≠\s*minh)',
            "hanoi": r'(h√†\s*n·ªôi|h√†\s*th√†nh)',
            "hue": r'(hu·∫ø|c·ªë\s*ƒë√¥)',
            "danang": r'(ƒë√†\s*n·∫µng)',
            "vungtau": r'(v≈©ng\s*t√†u|b√†\s*r·ªãa|b√£i\s*sau|b√£i\s*tr∆∞·ªõc)'
        }
        
        # T√¨m th√†nh ph·ªë trong query
        city_match = None
        for city, pattern in city_patterns.items():
            if re.search(pattern, query.lower()):
                city_match = city
                print(f"üîç Ph√°t hi·ªán th√†nh ph·ªë trong truy v·∫•n: {city}")
                break
                
        # Perform the search
        try:
            print(f"‚ÑπÔ∏è Performing similarity search in FAISS index...")
            
            # N·∫øu c√≥ th√†nh ph·ªë trong query, ∆∞u ti√™n k·∫øt qu·∫£ t·ª´ file t∆∞∆°ng ·ª©ng
            if city_match:
                # T√¨m ki·∫øm th√¥ng th∆∞·ªùng
                all_docs = self.vector_store.similarity_search(query, k=k+6)  # L·∫•y th√™m k·∫øt qu·∫£ ƒë·ªÉ l·ªçc
                
                # L·ªçc k·∫øt qu·∫£ theo th√†nh ph·ªë
                priority_docs = []
                other_docs = []
                
                city_file_patterns = {
                    "hcm": r'vietnam_hochiminh\.txt$',
                    "hanoi": r'vietnam_hanoi\.txt$',
                    "hue": r'vietnam_hue\.txt$',
                    "danang": r'vietnam_danang\.txt$',
                    "vungtau": r'vietnam_vungtau\.txt$'
                }
                
                pattern = city_file_patterns[city_match]
                print(f"üîç ∆Øu ti√™n t√¨m ki·∫øm t·ª´ file kh·ªõp v·ªõi m·∫´u: {pattern}")
                
                for doc in all_docs:
                    source = doc.metadata.get("source", "")
                    if re.search(pattern, source, re.IGNORECASE):
                        priority_docs.append(doc)
                        print(f"‚úÖ T√¨m th·∫•y t√†i li·ªáu ∆∞u ti√™n t·ª´ ngu·ªìn: {source}")
                    else:
                        other_docs.append(doc)
                
                # K·∫øt h·ª£p k·∫øt qu·∫£, ∆∞u ti√™n file th√†nh ph·ªë
                docs = priority_docs + other_docs
                docs = docs[:k]  # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng k·∫øt qu·∫£
                print(f"‚úÖ ƒê√£ ∆∞u ti√™n {len(priority_docs)} t√†i li·ªáu ph√π h·ª£p v·ªõi th√†nh ph·ªë {city_match}")
            else:
                # T√¨m ki·∫øm th√¥ng th∆∞·ªùng n·∫øu kh√¥ng c√≥ th√†nh ph·ªë trong query
                docs = self.vector_store.similarity_search(query, k=k)
                
            print(f"‚úÖ Found {len(docs)} relevant document chunks.")
            print("--- Search Finished ---")
            return docs
        except Exception as e:
            print(f"‚ùå Error during similarity search: {e}")
            print("--- Search Failed ---")
            return []